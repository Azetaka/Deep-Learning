{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w491pA49yoEo"
      },
      "source": [
        "**Avant de débuter ce TP** :\n",
        "\n",
        "1. **Changez le type d'exécution sur Google Colab** : `Exécution > Modifiez le type d'exécution > T4 GPU`\n",
        "2. **Installez les paquets ci-dessous** :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTFc6OivyoEq",
        "outputId": "775c1dc5-2adf-4d89-c2f3-9bddf48728a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lightning\n",
            "  Downloading lightning-2.5.5-py3-none-any.whl.metadata (39 kB)\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: PyYAML<8.0,>5.4 in /usr/local/lib/python3.12/dist-packages (from lightning) (6.0.3)\n",
            "Requirement already satisfied: fsspec<2027.0,>=2022.5.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning) (2025.3.0)\n",
            "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: packaging<27.0,>=20.0 in /usr/local/lib/python3.12/dist-packages (from lightning) (25.0)\n",
            "Requirement already satisfied: torch<4.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from lightning) (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.12/dist-packages (from lightning) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<6.0,>4.5.0 in /usr/local/lib/python3.12/dist-packages (from lightning) (4.15.0)\n",
            "Collecting pytorch-lightning (from lightning)\n",
            "  Downloading pytorch_lightning-2.5.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning) (3.13.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (75.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.4.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (1.22.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<4.0,>=2.1.0->lightning) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<4.0,>=2.1.0->lightning) (3.0.3)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (3.10)\n",
            "Downloading lightning-2.5.5-py3-none-any.whl (828 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m828.5/828.5 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Downloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Downloading pytorch_lightning-2.5.5-py3-none-any.whl (832 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m832.4/832.4 kB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchinfo, lightning-utilities, torchmetrics, pytorch-lightning, lightning\n",
            "Successfully installed lightning-2.5.5 lightning-utilities-0.15.2 pytorch-lightning-2.5.5 torchinfo-1.8.0 torchmetrics-1.8.2\n"
          ]
        }
      ],
      "source": [
        "! pip install lightning torchmetrics torchinfo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KV6JSJY7yoEr"
      },
      "source": [
        "3. Exécutez ce code pour supprimer quelques messages et avertissements éventuellement affichés."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "-sOxUFNIyoEr"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "logging.getLogger(\"lightning\").setLevel(logging.ERROR)\n",
        "logging.getLogger(\"lightning.pytorch.utilities.rank_zero\").setLevel(logging.WARNING)\n",
        "logging.getLogger(\"lightning.pytorch.accelerators.cuda\").setLevel(logging.WARNING)\n",
        "logger = logging.getLogger(\"lightning\")\n",
        "logger.propagate = False\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n",
        "warnings.filterwarnings(\"ignore\", \".*Missing logger folder.*\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjQplG-myoEs"
      },
      "source": [
        "# Classification d'images par réseau de neurones convolutif\n",
        "\n",
        "## Chargement des données\n",
        "\n",
        "Nous allons travailler sur un jeu de données appelé [EMNIST](https://www.westernsydney.edu.au/icns/resources/reproducible_research3/publication_support_materials2/emnist), et plus spécifiquement sur un sous-jeu de données constitué d'images en nuances de gris de lettres manuscrites.\n",
        "Pour ce faire, nous allons utiliser la classe [`torchvision.datasets.EMNIST()`](https://pytorch.org/vision/stable/generated/torchvision.datasets.EMNIST.html) pour télécharger et charger ce jeu de données."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "9Ie3py0SyoEs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision.datasets import EMNIST\n",
        "from torchvision.transforms import v2\n",
        "\n",
        "\n",
        "dataset_train = EMNIST(\n",
        "    root='data',\n",
        "    split='letters',\n",
        "    download=True,\n",
        "    train=True,\n",
        "    transform=v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]),\n",
        "    target_transform=lambda x: x - 1\n",
        ")\n",
        "\n",
        "dataset_val = EMNIST(\n",
        "    root='data',\n",
        "    split='letters',\n",
        "    download=True,\n",
        "    train=False,\n",
        "    transform=v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]),\n",
        "    target_transform=lambda x: x - 1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwYTpEKGyoEt"
      },
      "source": [
        "### Question 1\n",
        "\n",
        "**a)** Calculer la taille des jeux d'entraînement et de validation, c'est-à-dire le nombre d'images dans chacun des deux jeux.\n",
        "\n",
        "**b)** Calculer la taille d'une observation, c'est-à-dire la taille d'une image. On prendra pour acquis que toutes les observations ont la même taille (c'est bien le cas, vous n'avez pas à le vérifier)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38nJQJCEyoEt",
        "outputId": "0f93b092-6066-4cd9-eb5b-f4c058558e24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Taille du jeu d'entraînement : 124800\n",
            "Taille du jeu de validation : 20800\n",
            "Taille d'une observation : torch.Size([1, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "# a) Taille du jeu d'entraînement et validation\n",
        "print(f\"Taille du jeu d'entraînement : {len(dataset_train)}\")\n",
        "print(f\"Taille du jeu de validation : {len(dataset_val)}\")\n",
        "# b) Taille d'une observation\n",
        "print(f\"Taille d'une observation : {dataset_train[0][0].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oocURQrNyoEt"
      },
      "source": [
        "Il y a 26 classes dans ce jeu de données, correspondant aux 26 lettres de l'alphabet. Néanmoins, le jeu de données mélange les lettres majuscules et minuscules : des images pour les lettres *A* et *a* ont donc le même label.\n",
        "\n",
        "Le code ci-dessous permet d'afficher neuf observations tirées aléatoirement dans le jeu d'entraînement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "_eE5SsKOyoEu",
        "outputId": "1ae0892b-60ff-461d-cae0-92a7f9ce90da"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 9 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "figure = plt.figure(figsize=(8, 8))\n",
        "cols, rows = 3, 3\n",
        "for i in range(1, cols * rows + 1):\n",
        "    sample_idx = torch.randint(len(dataset_train), size=(1,)).item()\n",
        "    img, label = dataset_train[sample_idx]\n",
        "    figure.add_subplot(rows, cols, i)\n",
        "    plt.title(f\"{chr(65 + label)} ou {chr(97 + label)}\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(img.squeeze().T, cmap=\"gray\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5l9ouAMyoEu"
      },
      "source": [
        "### Question 2\n",
        "\n",
        "Vous avez peut-être remarqué le code suivant dans la création des jeux de données :\n",
        "\n",
        "```python\n",
        "transform=v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)],\n",
        "target_transform=lambda x: x - 1\n",
        "```\n",
        "\n",
        "En lisant la documentation de la classe [`torchvision.datasets.EMNIST()`](https://pytorch.org/vision/stable/generated/torchvision.datasets.EMNIST.html), vous verrez que :\n",
        "* l'argument `transform` permet de transformer l'image brute,\n",
        "* l'argument `target_transform` permet de transformer le label.\n",
        "\n",
        "**a)** Expliquez l'utilité du code transformant les entrées. Pour ce faire, pour la première observation du jeu d'entraînement, affichez le tenseur de l'entrée brute et le tenseur de l'entrée transformée. En particulier, comparez la taille, le type de données (*dtype*) et les valeurs minimale et maximale pour ces deux tenseurs.\n",
        "\n",
        "> **Remarque** : vous pouvez accéder aux données brutes de ce jeu de données grâce à l'attribut `data` (par exemple `dataset_train.data`)\n",
        "\n",
        "**b)** Expliquez l'utilité du code transformant les labels.\n",
        "\n",
        "> **Remarque** : vous pouvez accéder à la correspondance des labels bruts de ce jeu de données grâce à l'attribut `class_to_idx` (par exemple `dataset_train.class_to_idx`). Vous admettrez, sans le vérifier, qu'il n'y a aucun label manquant (`'N/A'`) dans les deux jeux de données."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCssFpWuyoEu",
        "outputId": "98414857-ac82-4691-a390-13e4fefca01e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'>\n"
          ]
        }
      ],
      "source": [
        "# a)Explication du code transformant les entrées\n",
        "  # Tenseur de l'entrée brute\n",
        "  # Tenseur de l'entrée transformée\n",
        "#print(dataset_train[0][0].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5ziK4uVyoEu"
      },
      "source": [
        "**Réponses** :\n",
        "    \n",
        "**a)** TODO\n",
        "\n",
        "**b)** TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfPH8a6VyoEu"
      },
      "source": [
        "### Question 3\n",
        "\n",
        "Calculez la distribution des classes sur les jeux d'entraînement et de validation. Est-ce que les classes sont équilibrées ? Est-ce que l'exactitude (*accuracy*) est une métrique adaptée pour évaluer la performance d'un modèle ? Vous pouvez utiliser, au choix, les fonctions [`numpy.unique()`](https://numpy.org/doc/stable/reference/generated/numpy.unique.html), [`numpy.bincount()`](https://numpy.org/doc/stable/reference/generated/numpy.bincount.html) ou [`torch.bincount()`](https://pytorch.org/docs/stable/generated/torch.bincount.html) pour vous aider."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5358LccyoEv",
        "outputId": "dde4de6a-7f44-4109-e519-6ce82774e191"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
            " 25 26]\n",
            "Nombre de classes : 26\n",
            "{np.int64(1): 4800, np.int64(2): 4800, np.int64(3): 4800, np.int64(4): 4800, np.int64(5): 4800, np.int64(6): 4800, np.int64(7): 4800, np.int64(8): 4800, np.int64(9): 4800, np.int64(10): 4800, np.int64(11): 4800, np.int64(12): 4800, np.int64(13): 4800, np.int64(14): 4800, np.int64(15): 4800, np.int64(16): 4800, np.int64(17): 4800, np.int64(18): 4800, np.int64(19): 4800, np.int64(20): 4800, np.int64(21): 4800, np.int64(22): 4800, np.int64(23): 4800, np.int64(24): 4800, np.int64(25): 4800, np.int64(26): 4800}\n"
          ]
        }
      ],
      "source": [
        "# Distribution des classes\n",
        "#torch.bincount(torch.tensor(dataset_train.targets - 1))\n",
        "import numpy as np\n",
        "# Liste des classes\n",
        "classes = np.unique(dataset_train.targets)\n",
        "print(\"Classes :\", classes)\n",
        "# Nombre de classes\n",
        "print(\"Nombre de classes :\", len(classes))\n",
        "dictionnaire = {}\n",
        "for i in classes:\n",
        "    dictionnaire[i] = np.count_nonzero(dataset_train.targets == i)\n",
        "print(dictionnaire)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Liste des classes\n",
        "classes_1 = np.unique(dataset_val.targets)\n",
        "print(\"Classes :\", classes)\n",
        "# Nombre de classes\n",
        "print(\"Nombre de classes :\", len(classes_1))\n",
        "dictionnaire = {}\n",
        "for i in classes:\n",
        "    dictionnaire[i] = np.count_nonzero(dataset_val.targets == i)\n",
        "print(dictionnaire)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDDME-tz8Mkc",
        "outputId": "5c10329c-1f36-44f2-d4fe-a2cbdd63da72"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
            " 25 26]\n",
            "Nombre de classes : 26\n",
            "{np.int64(1): 800, np.int64(2): 800, np.int64(3): 800, np.int64(4): 800, np.int64(5): 800, np.int64(6): 800, np.int64(7): 800, np.int64(8): 800, np.int64(9): 800, np.int64(10): 800, np.int64(11): 800, np.int64(12): 800, np.int64(13): 800, np.int64(14): 800, np.int64(15): 800, np.int64(16): 800, np.int64(17): 800, np.int64(18): 800, np.int64(19): 800, np.int64(20): 800, np.int64(21): 800, np.int64(22): 800, np.int64(23): 800, np.int64(24): 800, np.int64(25): 800, np.int64(26): 800}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ou fais tout simplement\n",
        "# Comptage du nombre d'images par classe\n",
        "train_counts = torch.bincount(dataset_train.targets-1)\n",
        "valid_counts = torch.bincount(dataset_val.targets-1)\n",
        "print(\"Distribution des classes - Entraînement :\", train_counts)\n",
        "print(\"Distribution des classes - Validation :\", valid_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNCZdkZz9ODV",
        "outputId": "054d5334-17a6-42bd-99b8-5a74c2b3b20a"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribution des classes - Entraînement : tensor([4800, 4800, 4800, 4800, 4800, 4800, 4800, 4800, 4800, 4800, 4800, 4800,\n",
            "        4800, 4800, 4800, 4800, 4800, 4800, 4800, 4800, 4800, 4800, 4800, 4800,\n",
            "        4800, 4800])\n",
            "Distribution des classes - Validation : tensor([800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800,\n",
            "        800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLK0MPKXyoEv"
      },
      "source": [
        "### Question 4\n",
        "\n",
        "Construisez les *dataloaders* pour les jeux d'entraînement et de validation. Vous utiliserez des lots de taille *64*. Mélangez les observations sur le jeu d'entraînement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "6yksSJTpyoEv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "X_train=dataset_train.data # Pour recuperer les données\n",
        "y_train=dataset_train.targets\n",
        "X_val=dataset_val.data\n",
        "y_val=dataset_val.targets\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "dataloader_train = DataLoader(TensorDataset(X_train, y_train), batch_size=64, shuffle=True)\n",
        "dataloader_val = DataLoader(TensorDataset(X_val, y_val), batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AEhuE9UyoEv"
      },
      "source": [
        "## Un premier réseau de neurones convolutif\n",
        "\n",
        "### Question 5\n",
        "\n",
        "Le code suivant est l'ébauche de la définition d'un réseau de neurones convolutif. Répondez aux questions suivantes :\n",
        "\n",
        "**a)** Quelles couches sont des couches de convolution ?\n",
        "\n",
        "**b)** Pourquoi le nombre de canaux en entrée est fixé à `1` dans `self.cv1` ?\n",
        "\n",
        "**c)** Indiquez, pour `self.cv1`, le nombre de canaux en sortie, la taille du noyau et la taille du pas.\n",
        "\n",
        "**d)** De quel type de couche de regroupement ce réseau est-il constitué ?\n",
        "\n",
        "**e)** Quelles sont les couches avec des paramètres entraînables ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "SYB9LMySyoEv"
      },
      "outputs": [],
      "source": [
        "import lightning as L\n",
        "from torchmetrics import Accuracy\n",
        "\n",
        "\n",
        "class CNN(L.LightningModule):  # La classe hérite de la classe lightning.LightningModule\n",
        "    def __init__(self):\n",
        "        \"\"\"Constructeur.\n",
        "\n",
        "        Dans le constructeur, on exécute le constructeur de la clase mère et on définit\n",
        "        toutes les couches et fonctions d'activation de notre réseau de neurones.\n",
        "        \"\"\"\n",
        "        super().__init__()  # Toujours exécuter le constructeur de la classe mère\n",
        "\n",
        "        # Initialisation des couches et fonctions d'activation\n",
        "        self.cv1 = torch.nn.Conv2d(in_channels=1, out_channels=4, kernel_size=5, padding='same') # Couche de convolution\n",
        "        self.relu1 = torch.nn.ReLU() # Fonction d'activation\n",
        "        self.pool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2) # Pooling\n",
        "        self.flatten = torch.nn.Flatten() # Couche flatten\n",
        "        self.fc2 = torch.nn.Linear(784, 64) #\n",
        "        self.relu2 = torch.nn.ReLU() # Fonction d'activation\n",
        "        self.fc3 = torch.nn.Linear(64, 26)\n",
        "        self.relu3 = torch.nn.ReLU() # Fonction d'activation\n",
        "\n",
        "        # Initialisation de la fonction de perte\n",
        "        self.loss = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        # Initialisation des métriques\n",
        "        self.accuracy_train = Accuracy(task=\"multiclass\", num_classes=26)\n",
        "        self.accuracy_val = Accuracy(task=\"multiclass\", num_classes=26)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Implémente la passe avant.\n",
        "\n",
        "        L'argument x est un tenseur correspondant soit à l'entrée une seule\n",
        "        observation soit aux entrées d'un lot d'observations.\n",
        "        \"\"\"\n",
        "        out = self.cv1(x)\n",
        "        out = self.relu1(out)\n",
        "        out = self.pool1(out)\n",
        "        out = self.flatten(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.relu2(out)\n",
        "        out = self.fc3(out)\n",
        "        out = self.relu3(out)\n",
        "        return out\n",
        "\n",
        "    def step(self, batch, dataset):\n",
        "        \"\"\"Effectue une étape.\n",
        "\n",
        "        Une étape consiste à passer d'un lot d'observations (l'argument batch)\n",
        "        à l'évaluation de la fonction de coût pour ce lot d'observations.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        batch : tuple\n",
        "            Un lot d'observations. Le premier élément du tuple est le lot\n",
        "            des entrées, le second est le lot des labels.\n",
        "\n",
        "        dataset : {\"training\", \"validation\"}\n",
        "            Jeu de données utilisé.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        loss : Tensor, shape = (1,)\n",
        "            La fonction de coût pour ce lot d'observations.\n",
        "        \"\"\"\n",
        "        X, y = batch  # X correspond aux images, y aux classes\n",
        "        logits = self(X)  # Passe avant, qui renvoie les logits\n",
        "        loss = self.loss(logits, y)  # Évaluation de la fonction de coût\n",
        "        y_pred = logits.argmax(1)  # Prédictions du modèle\n",
        "\n",
        "        if dataset == \"training\":\n",
        "            metric = self.accuracy_train\n",
        "            name = \"train\"\n",
        "            bar_step = True\n",
        "        else:\n",
        "            metric = self.accuracy_val\n",
        "            name = \"val\"\n",
        "            bar_step = False\n",
        "\n",
        "        acc = metric(y_pred, y)\n",
        "        self.log(f\"loss_{name}\", loss, prog_bar=bar_step, on_step=bar_step, on_epoch=True)\n",
        "        self.log(f\"accuracy_{name}\", acc, prog_bar=bar_step, on_step=bar_step, on_epoch=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, batch):\n",
        "        \"\"\"Effectue une étape d'entraînement.\"\"\"\n",
        "        return self.step(batch, \"training\")\n",
        "\n",
        "    def validation_step(self, batch):\n",
        "        \"\"\"Effectue une étape de validation.\"\"\"\n",
        "        return self.step(batch, \"validation\")\n",
        "\n",
        "    def on_train_start(self):\n",
        "        \"\"\"Code exécuté au début de l'entraînement.\"\"\"\n",
        "        string = f\"Version {self.trainer.logger.version}\"\n",
        "        print(f\"{string}\\n{'=' * len(string)}\\n\")\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        \"\"\"Code exécuté à la fin de chaque époque d'entraînement.\"\"\"\n",
        "        metrics = self.trainer.callback_metrics\n",
        "        string = (f\"\"\"\n",
        "            Époque {self.trainer.current_epoch + 1} / {self.trainer.max_epochs}\n",
        "            -------------------------------------------------\n",
        "            |     Jeu      | Fonction de perte | Exactitude |\n",
        "            | ------------ | ----------------- | ---------- |\n",
        "            | Entraînement |{metrics['loss_train'].item():^19.5f}|{metrics['accuracy_train'].item():^11.3%}|\n",
        "            |  Validation  |{metrics['loss_val'].item():^19.5f}|{metrics['accuracy_val'].item():^11.3%}|\n",
        "            -------------------------------------------------\n",
        "        \"\"\")\n",
        "        string = '\\n'.join([line.strip() for line in string.strip().split('\\n')])\n",
        "        print(string, \"\\n\")\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"Configure l'algorithme d'optimisation à utiliser.\"\"\"\n",
        "        optimizer = torch.optim.Adam(self.parameters())\n",
        "        return optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUkyiqxFyoEv"
      },
      "source": [
        "**Réponses** :\n",
        "\n",
        "*a)* Il n'y a qu'une seule couche de convolution, correspondant à l'attribut `cv1`.\n",
        "\n",
        "**b)** Les images en entrée sont en nuances de gris, il n'y a donc qu'un seul canal.\n",
        "\n",
        "**c)** Il y a 4 canaux en sortie, le noyau est de taille $5 \\times 5$ et le pas est de $1$.\n",
        "\n",
        "**d)** Il n'y a qu'une seule couche de regroupement, correspondant à l'attribut `pool1` : il s'agit d'un regroupement par le maximum.\n",
        "\n",
        "**e)** Les couches avec des paramètres entraînables sont la couche de convolution (`cv1`) et les couches linéaires (`fc2` et `fc3`). En effet, la fonction d'activation ReLU, la couche d'aplatissement et la couche de regroupement n'ont pas de paramètres entraînables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTTERkazyoEv"
      },
      "source": [
        "### Question 6\n",
        "\n",
        "En essayant d'afficher le résumé de l'architecture de notre modèle avec le code ci-dessous, une erreur va être levée. Identifiez le problème correspondant à cette erreur et corrigez-le. Indiquez, avec des mots, quel était le problème."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fM7VQfAfyoEv",
        "outputId": "52a03dbd-ead9-44d3-9bf7-4bffc0f496a1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "CNN                                      [64, 26]                  --\n",
              "├─Conv2d: 1-1                            [64, 4, 28, 28]           104\n",
              "├─ReLU: 1-2                              [64, 4, 28, 28]           --\n",
              "├─MaxPool2d: 1-3                         [64, 4, 14, 14]           --\n",
              "├─Flatten: 1-4                           [64, 784]                 --\n",
              "├─Linear: 1-5                            [64, 64]                  50,240\n",
              "├─ReLU: 1-6                              [64, 64]                  --\n",
              "├─Linear: 1-7                            [64, 26]                  1,690\n",
              "├─ReLU: 1-8                              [64, 26]                  --\n",
              "==========================================================================================\n",
              "Total params: 52,034\n",
              "Trainable params: 52,034\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 8.54\n",
              "==========================================================================================\n",
              "Input size (MB): 0.20\n",
              "Forward/backward pass size (MB): 1.65\n",
              "Params size (MB): 0.21\n",
              "Estimated Total Size (MB): 2.06\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "summary(CNN(), input_size=(64, 1, 28, 28))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5HReWCmyoEw"
      },
      "source": [
        "**Réponse** :\n",
        "Remplacer les 123 de self.fc2 = torch.nn.Linear(123, 64) par 784"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9PaCmMjyoEw"
      },
      "source": [
        "### Question 7\n",
        "\n",
        "Entraînez votre modèle pendant 10 époques. Comparez la performance du modèle à celle attendue pour un modèle trivial (qui prédirait toujours la même classe ou qui prédirait de façon totalement aléatoire les classes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450,
          "referenced_widgets": [
            "41a438141e18458dbf346a804a6778e9",
            "3012d4befb934588abdae81b39b32a6d",
            "f561f7f49ec44e19a66472b9c3079738",
            "d79aab521056465f8a1c0dd7e58fe0c1",
            "64ae950a144f49cf8eeb3a6281bd5f98",
            "3e348fa0bc204c2cb0868e08c44c4fc7",
            "c6a2ee588788443982f04588c13064e8",
            "a0c70ef5c034430d9c45a25b555b584a",
            "da502b5a67f148a09548d7bd52e9c467",
            "72ba4471c1c0427e819b3ca1f78b2e56",
            "5756fd3bacf04b58a3b366d1318bd25e"
          ]
        },
        "id": "FNP0HqX2yoEw",
        "outputId": "2bbc4bdb-330f-4d53-e6bf-fb215d198839"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "41a438141e18458dbf346a804a6778e9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Version 4\n",
            "=========\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Given groups=1, weight of size [4, 1, 5, 5], expected input[1, 64, 28, 28] to have 1 channels, but got 64 channels instead",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3308878020.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m trainer.fit(\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataloader_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    561\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_TunerExitException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0mmodel_connected\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         )\n\u001b[0;32m--> 598\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \u001b[0;31m# RUN THE TRAINER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1053\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_sanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detect_anomaly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1055\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1056\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unexpected state {self.state}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_training_epoch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_fetcher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    346\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautomatic_optimization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m                     \u001b[0;31m# in automatic optimization, there can only be one optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m                     \u001b[0mbatch_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautomatic_optimization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m                     \u001b[0mbatch_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_optimization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;31m# gradient update with accumulated gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsume_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py\u001b[0m in \u001b[0;36m_optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;31m# model hook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         call._call_lightning_module_hook(\n\u001b[0m\u001b[1;32m    271\u001b[0m             \u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0;34m\"optimizer_step\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py\u001b[0m in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[LightningModule]{pl_module.__class__.__name__}.{hook_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;31m# restore current_fx when nested context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/core/module.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \"\"\"\n\u001b[0;32m-> 1366\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer_closure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moptimizer_zero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/core/optimizer.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_strategy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mstep_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_on_after_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/strategies/strategy.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;31m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLightningModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setup_model_and_optimizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOptimizer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOptimizer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/plugins/precision/precision.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;34m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mclosure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_closure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     def _clip_gradients(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    514\u001b[0m                             )\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclosure\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/plugins/precision/precision.py\u001b[0m in \u001b[0;36m_wrap_closure\u001b[0;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \"\"\"\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mclosure_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_closure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mclosure_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py\u001b[0m in \u001b[0;36mclosure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mClosureResult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mstep_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosure_loss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py\u001b[0m in \u001b[0;36m_training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mtraining_step_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_strategy_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"training_step\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_training_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# unused hook - call anyway for backward compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py\u001b[0m in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[Strategy]{trainer.strategy.__class__.__name__}.{hook_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;31m# restore current_fx when nested context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/strategies/strategy.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_redirection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"training_step\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpost_training_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1048847009.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;34m\"\"\"Effectue une étape d'entraînement.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1048847009.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, batch, dataset)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \"\"\"\n\u001b[1;32m     67\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m  \u001b[0;31m# X correspond aux images, y aux classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Passe avant, qui renvoie les logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Évaluation de la fonction de coût\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Prédictions du modèle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1048847009.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[0msoit\u001b[0m \u001b[0maux\u001b[0m \u001b[0mentrées\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m'un lot d'\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \"\"\"\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    541\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             )\n\u001b[0;32m--> 543\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    544\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [4, 1, 5, 5], expected input[1, 64, 28, 28] to have 1 channels, but got 64 channels instead"
          ]
        }
      ],
      "source": [
        "from lightning.pytorch.callbacks import TQDMProgressBar\n",
        "from lightning.pytorch.loggers import CSVLogger\n",
        "\n",
        "\n",
        "model = CNN()\n",
        "\n",
        "trainer = L.Trainer(\n",
        "    max_epochs=10,\n",
        "    enable_model_summary=False,  # supprimer le résumé du modèle\n",
        "    logger=CSVLogger('.'),  # sauvegarder les résultats dans un fichier CSV\n",
        "    num_sanity_val_steps=0,  # ne pas effectuer d'étape de validation avant l'entraînement\n",
        "    callbacks=[TQDMProgressBar(refresh_rate=100)]  # mettre à jour la barre de progression tous les 100 lots\n",
        ")\n",
        "\n",
        "trainer.fit(\n",
        "    model=model,\n",
        "    train_dataloaders=dataloader_train,\n",
        "    val_dataloaders=dataloader_val\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6DqZUNiyoEw"
      },
      "source": [
        "**Réponse** : TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOcHuY3GyoEw"
      },
      "source": [
        "### Question 8\n",
        "\n",
        "Viusalisez les courbes de performance du modèle en utilisant la fonction `plot_loss_accuracy` définie ci-dessous. Vous pouvez utiliser l'argument `version` pour indiquer quelle version du modèle choisir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Jx5U8ffyoEw"
      },
      "outputs": [],
      "source": [
        "def plot_loss_accuracy(savedir='.', version=None):\n",
        "    \"\"\"Affiche les courbes de la fonction de perte et d'accuracy.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    savedir : str (default = '.')\n",
        "        Chemin où les résultats sont sauvegardés.\n",
        "\n",
        "    version : int or None (default = None)\n",
        "        Numéro de la version du modèle.\n",
        "    \"\"\"\n",
        "    # Récupère les résultats sous la forme d'un DataFrame\n",
        "    import os\n",
        "    import pandas as pd\n",
        "    if version is None:\n",
        "        version = max([\n",
        "            int(folder.split('version_')[1])\n",
        "            for folder in os.listdir(os.path.join(savedir, 'lightning_logs'))\n",
        "            if folder.startswith('version')\n",
        "        ])\n",
        "    df = pd.read_csv(os.path.join(savedir, 'lightning_logs', f'version_{version}', 'metrics.csv'))\n",
        "    df['epoch'] += 1  # On commence à compter à partir de 1\n",
        "\n",
        "    loss_train = df.dropna(subset='loss_train_epoch').set_index('epoch')['loss_train_epoch']\n",
        "    loss_val = df.dropna(subset='loss_val').set_index('epoch')['loss_val']\n",
        "\n",
        "    accuracy_train = df.dropna(subset='accuracy_train_epoch').set_index('epoch')['accuracy_train_epoch']\n",
        "    accuracy_val = df.dropna(subset='accuracy_val').set_index('epoch')['accuracy_val']\n",
        "\n",
        "    # Affiche les résultats\n",
        "    plt.figure(figsize=(13, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(loss_train.index, loss_train.to_numpy(), 'o-', label='Entraînement');\n",
        "    plt.plot(loss_val.index, loss_val.to_numpy(), 'o-', label='Validation');\n",
        "    plt.xlabel('Époque')\n",
        "    plt.ylabel('Fonction de perte')\n",
        "    plt.legend();\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(accuracy_train.index, accuracy_train.to_numpy(), 'o-', label='Entraînement');\n",
        "    plt.plot(accuracy_val.index, accuracy_val.to_numpy(), 'o-', label='Validation');\n",
        "    plt.xlabel('Époque')\n",
        "    plt.ylabel('Précision')\n",
        "    plt.legend();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVAcXEx4yoEw"
      },
      "outputs": [],
      "source": [
        "plot_loss_accuracy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2q7ji7O-yoEw"
      },
      "source": [
        "## Un deuxième réseau de neurones convolutif\n",
        "\n",
        "### Question 9\n",
        "\n",
        "Complétez la méthode `__init__()` de la classe `LeNet5` définie ci-dessous dont l'achitecture séquentielle est décrite ci-dessous, correspondant à l'architecture de [LeNet5](https://en.wikipedia.org/wiki/LeNet) :\n",
        "\n",
        "* une couche de convolution avec $6$ canaux en sortie, un noyau de taille $5 \\times 5$ et du rembourrage (*padding*) de telle que sorte l'image en sortie soit de la même taille que l'image en entrée.\n",
        "* la fonction d'activation sigmoïde.\n",
        "* une couche de regroupement par le maximum avec un noyau de taille $2 \\times 2$ et un pas de $2$.\n",
        "* une couche de convolution avec $16$ canaux en sortie, un noyau de taille $5 \\times 5$ et sans rembourrage.\n",
        "* la fonction d'activation sigmoïde.\n",
        "* une couche de regroupement par le maximum avec un noyau de taille $2 \\times 2$ et un pas de $2$.\n",
        "* une couche d'aplatissement (*flatten*).\n",
        "* une couche linéaire avec une entrée de taille $400$ et une sortie de taille $256$.\n",
        "* la fonction d'activation sigmoïde.\n",
        "* une couche linéaire avec une sortie de taille $128$.\n",
        "* la fonction d'activation sigmoïde.\n",
        "* une couche linéaire avec une sortie de taille $26$.\n",
        "\n",
        "Voici les liens vers les documentations des classes correspondantes :\n",
        "[`torch.nn.Conv2d()`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html),\n",
        "[`torch.nn.Sigmoid()`](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html),\n",
        "[`torch.nn.MaxPool2d()`](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html),\n",
        "[`torch.nn.Flatten()`](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html) et\n",
        "[`torch.nn.Linear()`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyeZ0obVyoEw"
      },
      "outputs": [],
      "source": [
        "class LeNet5(L.LightningModule):  # La classe hérite de la classe lightning.LightningModule\n",
        "    def __init__(self):\n",
        "        \"\"\"Constructeur.\n",
        "\n",
        "        Dans le constructeur, on exécute le constructeur de la clase mère et on définit\n",
        "        toutes les couches et fonctions d'activation de notre réseau de neurones.\n",
        "        \"\"\"\n",
        "        super().__init__()  # Toujours exécuter le constructeur de la classe mère\n",
        "\n",
        "        ### BEGIN TODO ###\n",
        "        # Initialisation du modèle\n",
        "        #### END TODO ####\n",
        "\n",
        "        # Initialisation de la fonction de perte\n",
        "        self.loss = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        # Initialisation des métriques\n",
        "        self.accuracy_train = Accuracy(task=\"multiclass\", num_classes=26)\n",
        "        self.accuracy_val = Accuracy(task=\"multiclass\", num_classes=26)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Implémente la passe avant.\n",
        "\n",
        "        L'argument x est un tenseur correspondant soit à l'entrée une seule\n",
        "        observation soit aux entrées d'un lot d'observations.\n",
        "        \"\"\"\n",
        "        ### BEGIN TODO ###\n",
        "        # y =\n",
        "        #### END TODO ####\n",
        "        return y\n",
        "\n",
        "    def step(self, batch, dataset):\n",
        "        \"\"\"Effectue une étape.\n",
        "\n",
        "        Une étape consiste à passer d'un lot d'observations (l'argument batch)\n",
        "        à l'évaluation de la fonction de coût pour ce lot d'observations.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        batch : tuple\n",
        "            Un lot d'observations. Le premier élément du tuple est le lot\n",
        "            des entrées, le second est le lot des labels.\n",
        "\n",
        "        dataset : {\"training\", \"validation\"}\n",
        "            Jeu de données utilisé.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        loss : Tensor, shape = (1,)\n",
        "            La fonction de coût pour ce lot d'observations.\n",
        "        \"\"\"\n",
        "        X, y = batch  # X correspond aux images, y aux classes\n",
        "        logits = self(X)  # Passe avant, qui renvoie les logits\n",
        "        loss = self.loss(logits, y)  # Évaluation de la fonction de coût\n",
        "        y_pred = logits.argmax(1)  # Prédictions du modèle\n",
        "\n",
        "        if dataset == \"training\":\n",
        "            metric = self.accuracy_train\n",
        "            name = \"train\"\n",
        "            bar_step = True\n",
        "        else:\n",
        "            metric = self.accuracy_val\n",
        "            name = \"val\"\n",
        "            bar_step = False\n",
        "\n",
        "        acc = metric(y_pred, y)\n",
        "        self.log(f\"loss_{name}\", loss, prog_bar=bar_step, on_step=bar_step, on_epoch=True)\n",
        "        self.log(f\"accuracy_{name}\", acc, prog_bar=bar_step, on_step=bar_step, on_epoch=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, batch):\n",
        "        \"\"\"Effectue une étape d'entraînement.\"\"\"\n",
        "        return self.step(batch, \"training\")\n",
        "\n",
        "    def validation_step(self, batch):\n",
        "        \"\"\"Effectue une étape de validation.\"\"\"\n",
        "        return self.step(batch, \"validation\")\n",
        "\n",
        "    def on_train_start(self):\n",
        "        \"\"\"Code exécuté au début de l'entraînement.\"\"\"\n",
        "        string = f\"Version {self.trainer.logger.version}\"\n",
        "        print(f\"{string}\\n{'=' * len(string)}\\n\")\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        \"\"\"Code exécuté à la fin de chaque époque d'entraînement.\"\"\"\n",
        "        metrics = self.trainer.callback_metrics\n",
        "        string = (f\"\"\"\n",
        "            Époque {self.trainer.current_epoch + 1} / {self.trainer.max_epochs}\n",
        "            -------------------------------------------------\n",
        "            |     Jeu      | Fonction de perte | Exactitude |\n",
        "            | ------------ | ----------------- | ---------- |\n",
        "            | Entraînement |{metrics['loss_train'].item():^19.5f}|{metrics['accuracy_train'].item():^12.3%}|\n",
        "            |  Validation  |{metrics['loss_val'].item():^19.5f}|{metrics['accuracy_val'].item():^12.3%}|\n",
        "            -------------------------------------------------\n",
        "        \"\"\")\n",
        "        string = '\\n'.join([line.strip() for line in string.strip().split('\\n')])\n",
        "        print(string, \"\\n\")\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"Configure l'algorithme d'optimisation à utiliser.\"\"\"\n",
        "        optimizer = torch.optim.Adam(self.parameters())\n",
        "        return optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpOWs9qryoEw"
      },
      "source": [
        "### Question 10\n",
        "\n",
        "Affichez un résumé de cette architecture. Combien de paramètres entraînables a cette architecture ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwmxpOuGyoEx"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOCgjuWpyoEx"
      },
      "source": [
        "**Réponse** : TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Agh7K6dZyoEx"
      },
      "source": [
        "### Question 11\n",
        "\n",
        "Entraînez votre nouveau réseau de neurones pendant 10 époques. Comparez sa performance à celle du modèle précédent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5F_PNxDyoEx"
      },
      "outputs": [],
      "source": [
        "model_lenet = LeNet5()\n",
        "\n",
        "trainer_lenet = L.Trainer(\n",
        "    max_epochs=10,\n",
        "    enable_model_summary=False,  # supprimer le résumé du modèle\n",
        "    logger=CSVLogger('.'),  # sauvegarder les résultats dans un fichier CSV\n",
        "    num_sanity_val_steps=0,  # ne pas effectuer d'étape de validation avant l'entraînement\n",
        "    callbacks=[TQDMProgressBar(refresh_rate=100)]  # mettre à jour la barre de progression tous les 100 lots\n",
        ")\n",
        "\n",
        "trainer_lenet.fit(\n",
        "    model=model_lenet,\n",
        "    train_dataloaders=dataloader_train,\n",
        "    val_dataloaders=dataloader_val\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LgXgslWyoEx"
      },
      "outputs": [],
      "source": [
        "plot_loss_accuracy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ul2UFig-yoEx"
      },
      "source": [
        "**Réponse** : TODO"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "41a438141e18458dbf346a804a6778e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3012d4befb934588abdae81b39b32a6d",
              "IPY_MODEL_f561f7f49ec44e19a66472b9c3079738",
              "IPY_MODEL_d79aab521056465f8a1c0dd7e58fe0c1"
            ],
            "layout": "IPY_MODEL_64ae950a144f49cf8eeb3a6281bd5f98"
          }
        },
        "3012d4befb934588abdae81b39b32a6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e348fa0bc204c2cb0868e08c44c4fc7",
            "placeholder": "​",
            "style": "IPY_MODEL_c6a2ee588788443982f04588c13064e8",
            "value": "Epoch 0:   0%"
          }
        },
        "f561f7f49ec44e19a66472b9c3079738": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0c70ef5c034430d9c45a25b555b584a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_da502b5a67f148a09548d7bd52e9c467",
            "value": 0
          }
        },
        "d79aab521056465f8a1c0dd7e58fe0c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72ba4471c1c0427e819b3ca1f78b2e56",
            "placeholder": "​",
            "style": "IPY_MODEL_5756fd3bacf04b58a3b366d1318bd25e",
            "value": " 0/1950 [00:00&lt;?, ?it/s]"
          }
        },
        "64ae950a144f49cf8eeb3a6281bd5f98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "3e348fa0bc204c2cb0868e08c44c4fc7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6a2ee588788443982f04588c13064e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0c70ef5c034430d9c45a25b555b584a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da502b5a67f148a09548d7bd52e9c467": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "72ba4471c1c0427e819b3ca1f78b2e56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5756fd3bacf04b58a3b366d1318bd25e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}